---
title: "Stochastic learning methods"
subtitle: "Logistic regression"
author: "Georgios P. Karagiannis @ MATH3431 Machine Learning and Neural Networks III"
output:
  html_notebook: 
    number_sections: true
  word_document: default
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: default
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{amsmath}
---

<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2023 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Associate Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Machine Learning and Neural Networks III (MATH3431) -->
<!-- which is the material of the course (MATH3431 Machine Learning and Neural Networks III) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Machine_Learning_and_Neural_Networks_III_Epiphany_2023  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->



[Back to README](https://github.com/georgios-stats/Machine_Learning_and_Neural_Networks_III_Epiphany_2023/tree/main/Computer_practical#aim)

```{r}
rm(list=ls())
```


---

***Aim***

Students will become able to:  

+ practice in R,  

+ implement GD, batch/online SGD, AdaGrad, SGD with projection, SVRG algorithms in R with possibility to extend to other programming languadges  

+ refresh logistic regression, with Ridge penalty  

---

***Reading material***


+ Lecture notes:  
    + Handouts 0, 1, 2, and 3 

+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   

+ Reference of *rmarkdown* (optional):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)

+ Reference for *Latex* (optional):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)  

---

***New software***   

+ R package `base` functions:    
    + `set.seed{base}` 

+ R package `nloptr` functions:    
    + `nloptr{nloptr}` 
    
+ R package `numDeriv` functions:    
    + `grad{numDeriv}` 

```{r}
# call libraries
library(numDeriv)
library(nloptr)
```

---


```{r, results="hide"}
# Load R package for printing
library(knitr)
```

```{r, results="hide"}
# Set a seed of the randon number generator
set.seed(2023)
```


# Application: Logistic regression    {-}

Consider the Logistic regression problem with input $x\in\mathbb{R}$ and output/labels $y\in\{0,1\}$. The prediction rule is $$h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}$$, where $w\in\mathbb{R}^{2}$ is the unknown parameter we wish to learn, and hence we can consider that the hypothesis class is $$\mathcal{H}=\{w\in\mathbb{R}^{2}\}$$.  

Consider there is available a dataset  $$\mathcal{S}_{n}=\left\{ z_{i}=\left(y_{i},x_{i}\right)\right\} _{i=1}^{n}$$ , with $y_{i}\in\{0,1\}$ and $x_{i}\in\mathbb{R}$.  Recall that the sampling distribution is  
$$
y|w \sim \text{Bernulli}(h_{w}(x)) \\
h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}
$$

The dataset $\mathcal{S}_{n}$ is  generated from the data generation probability $g(\cdot)$ provided below as a routine. We pretend that we do not know $g(\cdot)$. 

```{r}
data_generating_model <- function(n,w) {
  z <- rep( NaN, times=n*2 )
  z <- matrix(z, nrow = n, ncol = 2)
  z[,1] <- runif(n, min = -10, max = 10)
  p <- w[1] + w[2]*z[,1] 
  p <- exp(p) / (1+exp(p))
  z[,2] <- rbinom(n, size = 1, prob = p)
  return(z)
}
```


Let the dataset $\mathcal{S}_{n}$ has size $n=500$.  Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  
```{r}
set.seed(2023)
n_obs <- 500
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true) 
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```



The likelihood function is 
$$
f\left(y|w\right)=\prod_{i=1}^{n}\left(h_{i}\right)^{y_{i}}\left(1-h_{i}\right)^{1-y_{i}}
$$

We consider a loss function as 
$$
\ell\left(w,z=\left(x,y\right)\right)=-y\log\left(h(x)\right)-\left(1-y\right)\log\left(1-h(x)\right)
$$

The Risk function under the data generation model $g$ is 
$$
\begin{align*}
R_{g}\left(w\right)= & \text{E}\left(\ell\left(w,z=\left(x,y\right)\right)\right)\\
= & \text{E}\left(-y\log\left(h\left(w;x\right)\right)-\left(1-y\right)\log\left(1-h\left(w;x\right)\right)\right)
\end{align*}
$$


## Task (given) {-}

The prediction rule is $$h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}$$, where $w\in\mathbb{R}^{2}$

Write a function `prediction_rule(x,w)' that returns the rule $h$ where $x$ is the input argument and $w$ is the unknown parameter.

```{r}
prediction_rule <- function(x,w) {
  h <- w[1]+w[2]*x
  h <- exp(h) / (1.0 + exp(h) )
  return (h)
}
```



## Task (given) {-}


The loss function as 
$$
\ell\left(w,z=\left(x,y\right)\right)=-y\log\left(h(x)\right)-\left(1-y\right)\log\left(1-h(x)\right)
$$
Write a function `loss_fun(w,z)' that computes the loss function, where $z=(x,y)$ is one example (observation) and $w$ is the unknown parameter. 

```{r}
loss_fun <- function(w,z) {
  x = z[1]
  y = z[2]
  h <- prediction_rule(x,w)
  ell <- -y*log(h) - (1-y)*log(1-h)
  return (ell)
}
```

## Task (given) {-}

The Empirical risk function is
$$
\begin{align*}
\hat{R}_{S}\left(w\right) & \frac{1}{n}\sum_{i=1}^{n}\ell\left(w,z_{i}=\left(x_{i},y_{i}\right)\right)\\
= & -\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}\log\left(h(w;x_{i})\right)+\left(1-y\right)\log\left(1-h(w;x_{i})\right)\right)
\end{align*}
$$

Write a function `empirical_risk_fun(w,z,n)' that computes the empirical risk, where $z=(x,y)$ is an example, $w$ is the unknown parameter, and $n$ is the data size. 

```{r}
empirical_risk_fun <- function(w,z,n) {
  x = z[,1]
  y = z[,2]
  R <- 0.0
  for (i in 1:n) {
    R <- R + loss_fun(w,z[i,])
  }
  R <- R / n
  return (R)
}
```

## Task (given) {-}

Write a function `learning_rate(t,t0)' that computes the learning rate sequence $$\eta_{t}=\frac{t_0}{t}$$ where $t$ is the iteration stage and $t_0$ is a constant. Use $t_0=3$ as default value.  

```{r}
learning_rate <-function(t,t0=3) {
  eta <- t0 / t
  return( eta )
}
```

## Task (given) {-}

Code the function 'grad_loss_fun(w,z)' that returns the gradient of the loss function at parameter value w, and at example value z. 

```{r}
grad_loss_fun <- function(w,z) {
  x = z[1]
  y = z[2]
  h <- prediction_rule(x,w)
  grd <- c(h-y, (h-y)*x)
  return (grd)
}
```

## Task (given) {-}

Code the function 'grad_risk_fun <- function(w,z,n)' that returns the gradient of the risk function at parameter value $w$, and using the dataset $z$ of size $n\times 2$ .  

```{r}
grad_risk_fun <- function(w,z,n) {
  grd <- 0.0
  for (i in 1:n) {
    grd <- grd + grad_loss_fun(w,z[i,])
  }
  grd <- grd / n
  return (grd)
}

```


## Task (Computer practical exercise)  

Compute the gradient of the empirical risk function at point $w=(-0.1,1.5)^\top$. Use the whole dataset $\{z_{i};i=1,...,n\}$ (set of examples). Do this by using the command 'grad_risk_fun' provided above.

```{r}
w <- c(-0.1,1.5)
gr <- grad_risk_fun (w, z=z_obs, n=n_obs) 
gr
```


## Task (Computer practical exercise)  

Compute the gradient of the empirical risk function at point $w=(-0.3,3)^\top$. Use the whole dataset $\{z_{i};i=1,...,n\}$ (set of examples).   Do this by using the function 'grad{numDeriv}' from the R package numDeriv. 

E.g., you can use it as numDeriv::grad( fun, w ). You can try ?grad for more info.

```{r}
w <- c(-0.1,1.5)
#
erf_fun <- function(w, z = z_obs, n=n_obs) {
  return( empirical_risk_fun(w, z, n) ) 
}
#
gr <- numDeriv::grad( erf_fun, w )
#
gr
```

# Task: Gradient descent

## Task (Computer practical exercise)  

Code a Gradient Descent (GD) algorithm with constant learning rate $\eta_{t}=0.1$ that returns the chain of all the  $\{w^{(t)}\}$ produced. 

The termination criterion is when the total number of iterations excesses $T=1000$. 

Seed with $w^{(0)}=(-0.3,3)^\top$.   

You may use the R function 'grad{numDeriv}' to numerically compute the gradient; e.g. numDeriv::grad( erf_fun, w ) . Try ?grad for more info.

Alternatively, you can you the corresponding function grad_risk_fun provide above. 


```{r}
eta <- 0.1
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  erf_fun <- function(w, z = z_obs, n=n_obs) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * numDeriv::grad( erf_fun, w )
  w_chain <- rbind(w_chain, w)
  # step 2: check for termination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
```


```{r}
# eta <- 0.1
# Tmax <- 1000
# w_seed <- c(-0.1,1.5)
# w <- w_seed
# w_chain <- c()
# Qstop <- 0 
# t <- 0
# while ( Qstop == 0 ) {
#   # counter
#   t <- t +  1
#   #eta <- learning_rate( t )
#   # step 1: update 
#   w <- w - eta * grad_risk_fun( w, z_obs, n_obs )
#   w_chain <- rbind(w_chain, w)
#   # step 2: check for termination terminate
#   if ( t>= Tmax ) {
#     Qstop <- 1
#   }
# }
```



## Task (Computer practical exercise)  

Plot the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$


```{r}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
#
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```


## Task (Computer practical exercise)  

Repeat previous task to run GD by changing the values of $\eta$ in the range $(0.001,1.0)$. 

Check how the algorithm behaves by ploting the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$.  

If necessary change the termination criterio to consider more or less iterations.


```{r}
eta <- 0.5
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = z_obs, n=n_obs) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * numDeriv::grad( erf_fun, w )
  #w <- w - eta * grad_risk_fun( w, z_obs, n_obs )
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```

## Task (Computer practical exercise)  

Repeat previous couple of tasks to run GD by using a learning rate sequence of the form $\eta_t = t0/t$ for different values of $t_0>0$ that you will choose.  

Check how the algorithm behaves by ploting the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$.  

If necessary change the termination criterion to consider more or less iterations.


```{r}
learning_rate <- function(t,t0) {
  return(t0/t)
}
t0<- 10
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  eta <- learning_rate( t, t0 )
  erf_fun <- function(w, z = z_obs, n=n_obs) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * numDeriv::grad( erf_fun, w )
  #w <- w - eta * grad_risk_fun( w, z_obs, n_obs )
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```

# Batch Stochastic Gradient Descent 

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stored in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task (Exercise for homework practice)   

Code a batch Stochastic Gradient Descent (GD) algorithm with learning rate $\eta_{t}=0.01$ and batch size $m=10$ that returns the chain of $\{w^{(t)}\}$.  

The batch sampling may be performed as a sampling with replacement (see ?sample.int).  

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
m <- 10
eta <- 0.1
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  }
  else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * numDeriv::grad( erf_fun, w )
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```


## Task (Exercise for homework practice)   

Repeat the task above by experimenting and changing the values of the learning rate $\eta$ and that of the batch size $m$.  

Plot the produced chains of $\{w^{(t)}\}$. 

What is the impact of the the learning rate $eta$ and that of the batch size $m$ to the noise and the speed of the convergence ?


```{r}
m <- 10
eta <- 0.1
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  }
  else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * numDeriv::grad( erf_fun, w )
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
#
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
#
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```



# AdaGrad  (Exercise for homework practice)   

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task (Exercise for homework practice)   

Code an online AdaGrad algorithm with constant$\eta_{t}=1.0$ and batch size $m=1$ that returns the chain of $\{w^{(t)}\}$. 

Use $\epsilon=10^{-6}$

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
m <- 1
eta <- 1.0
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
G <- rep(0.0,times=length(w))
eps <- 10^(-6)
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  } else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  g <- numDeriv::grad( erf_fun, w )
  G <- G + g^2
  w <- w - eta * (1.0/sqrt(G+eps)) * g
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```


## Task (Exercise for homework practice)   

Code an batch AdaGrad algorithm with $\eta_{t}$ and batch size $m$ that returns the chain of $\{w^{(t)}\}$. 

Experiment by changing the values of $\eta_{t}$  and $m$. See how the oscillations and the convergence of the chain$\{w^{(t)}\}$ behaves.

Use $\epsilon=10^{-6}$

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
m <- 100
eta <- 1.0
Tmax <- 1000
w_seed <- c(-0.1,1.5)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
G <- rep(0.0,times=length(w))
eps <- 10^(-6)
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  } else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  g <- numDeriv::grad( erf_fun, w )
  G <- G + g^2
  w <- w - eta * (1.0/sqrt(G+eps)) * g
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```



# Batch Stochastic Gradient Descent with projection  (Exercise for homework practice)   

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

Assume that the hypothesis class is restricted such as $\mathcal{H}=\{w\in\mathbb{R}^{2}:|w|_2 \le 2.0\} $

You may use the function 

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```


## Task (Exercise for homework practice)   


Compute $w^{*}$ from the minimization problem 

$$
w^{*}=\arg\min_{w\in\mathcal{H}}\left( F(w) \right)
$$
where $F(w)=\left\Vert w-w' \right\Vert$, $w'=(-0.1,0.3)^\top$ and  $\mathcal{H}=\{w\in\mathbb{R}^{2}:|w|_2 \le 2.0\} $. 

Use the function 'nloptr{nloptr}' from the R package 'nloptr'. Try ?nloptr for more information.  

A set of sufficient arguments for the function to run is given below.

```{r}
# out <- nloptr(x0=..,   
#               eval_f=..., #   
#               eval_grad_f=...,  
#               eval_g_ineq = ...,  
#               eval_jac_g_ineq = ...,   
#               w_now=...,  
#               opts = list("algorithm" = "NLOPT_LD_MMA",  
#                           "xtol_rel"=1.0e-8)  
# out$solution
```


The following functions are provided.

```{r}
#
boundary <- 2.0 # this is the value |w|_{2}^{2} <= boundary
# auxiliary functions to compute the projection
eval_f0 <- function( w_proj, w_now ){ 
  return( sqrt(sum((w_proj-w_now)^2)) )
}
eval_grad_f0 <- function( w, w_now ){ 
  return( c( 2*(w[1]-w_now[1]), 2*(w[2]-w_now[2]) ) )
}
eval_g0 <- function( w_proj, w_now) {
  return( sum(w_proj^2) -(boundary)^2 )
}
eval_jac_g0 <- function( x, w_now ) {
  return(   c(2*w[1],2*w[2] )  )
}
```

Write your code below

```{r}
#
w <- c(-0.1,0.3)
#
out <- nloptr(x0=c(0.0,0.0),
            eval_f=eval_f0,
            eval_grad_f=eval_grad_f0,
            eval_g_ineq = eval_g0,
            eval_jac_g_ineq = eval_jac_g0, 
            w_now=w,
            opts = list("algorithm" = "NLOPT_LD_MMA",
                        "xtol_rel"=1.0e-8) 
            )
out$solution
```


## Task (Exercise for homework practice)   

Code a batch Stochastic Gradient Descent (GD) algorithm with learning rate $\eta_{t}=0.01$, batch size $m=1$ and projection to $\mathcal{H}=\{w:|w|_2\le 2.0\}$, that returns the chain of $\{w^{(t)}\}$. 

The batch sampling may be performed as a sampling with replacement (see ?sample.int). 

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.1,0.3)^\top$.   

You may use the function 'nloptr{nloptr}' from the R package 'nloptr'. Try ?nloptr for more information.


```{r}
#
boundary <- 2.0 # this is the value |w|_{2}^{2} <= boundary
# auxiliary functions to compute the projection
eval_f0 <- function( w_proj, w_now ){ 
  return( sqrt(sum((w_proj-w_now)^2)) )
}
eval_grad_f0 <- function( w, w_now ){ 
  return( c( 2*(w[1]-w_now[1]), 2*(w[2]-w_now[2]) ) )
}
eval_g0 <- function( w_proj, w_now) {
  return( sum(w_proj^2) -(boundary)^2 )
}
eval_jac_g0 <- function( x, w_now ) {
  return(   c(2*w[1],2*w[2] )  )
}
```


```{r}
m <- 1
eta <- 0.01
Tmax <- 1000
w_seed <- c(-0.1,0.3)
w <- w_seed
w_chain <- c()
Qstop <- 0 
t <- 0
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  } else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * numDeriv::grad( erf_fun, w )
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  # step 1.5 projection
  out <- nloptr(x0=c(0.0,0.0),
              eval_f=eval_f0,
              eval_grad_f=eval_grad_f0,
              eval_g_ineq = eval_g0,
              eval_jac_g_ineq = eval_jac_g0, 
              w_now=w,
              opts = list("algorithm" = "NLOPT_LD_MMA",
                          "xtol_rel"=1.0e-8),
  )
  w <- out$solution
  # record
  w_chain <- rbind(w_chain, w)
  # step 2: check for rtermination terminate
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```

---

# Stochastic Variance Reduced Gradient Descent (SVRG)  

## Task  (Exercise for homework practice)   

Code a Stochastic Variance Reduced Gradient (SVRG) Descent algorithm with learning rate $\eta_{t}=0.01$ and batch size $m=1$ (namely the online SGD version of it) that returns the chain of $\{w^{(t)}\}$.  

Consider $\kappa=100$ as the SVRG parameter controlling the number of snapshots.   

The sampling may be performed as a sampling with replacement (see ?sample.int).  

The termination criterion is when the total number of iterations excesses $T=1000$. Seed with $w^{(0)}=(-0.3,3)^\top$.  

Produce the trace plots of the produced chains $\{w^(t)\}$.  


```{r}
m <- 1
eta <- 0.05
Tmax <- 1000
kappa <- 10000
Qstop <- 0 
t <- 0
#
#seeds
w_seed <- c(-0.1,0.3)
w <- w_seed
w_chain <- c()
cv_w <- w  
erf_fun <- function(w, z = z_obs, n=n_obs) {
  return( empirical_risk_fun(w, z, n) ) 
}
cv_grad_risk <- numDeriv::grad( erf_fun, cv_w ) #control variate
#
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  }
  else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * (numDeriv::grad( erf_fun, w ) -numDeriv::grad( erf_fun, cv_w ) +cv_grad_risk )
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  # record
  w_chain <- rbind(w_chain, w)
  # control variate step
  if ( (t %% kappa) == 0) {
    cv_w <- w  
    erf_fun <- function(w, z = z_obs, n=n_obs) {
      return( empirical_risk_fun(w, z, n) ) 
    }
    cv_grad_risk <- numDeriv::grad( erf_fun, cv_w ) #controle variate
  }
  # step 2: check for termination 
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```

## Task (Exercise for homework practice)   

Repeat the above task by changing the parameters $\kappa$ in order to see how changing $\kappa$ affects the convergence and the noise of the chain. 



```{r}
m <- 1
eta <- 0.1
Tmax <- 1000
kappa <- 10000
Qstop <- 0 
t <- 0
#
#seeds
w_seed <- c(-0.1,0.3)
w <- w_seed
w_chain <- c()
cv_w <- w  
erf_fun <- function(w, z = z_obs, n=n_obs) {
  return( empirical_risk_fun(w, z, n) ) 
}
cv_grad_risk <- numDeriv::grad( erf_fun, cv_w ) #control variate
#
while ( Qstop == 0 ) {
  # counter
  t <- t +  1
  # step 1: update  
  J <- sample.int(n = n_obs, size = m, replace = TRUE)
  if (m==1) {
    zbatch <- matrix(z_obs[J,],1,2)
  }
  else {
    zbatch <- z_obs[J,]
  }
  #eta <- learning_rate( t )
  erf_fun <- function(w, z = zbatch, n=m) {
    return( empirical_risk_fun(w, z, n) ) 
  }
  w <- w - eta * (numDeriv::grad( erf_fun, w ) -numDeriv::grad( erf_fun, cv_w ) +cv_grad_risk )
  #w <- w - eta * grad_risk_fun( w, zbatch, m )
  # record
  w_chain <- rbind(w_chain, w)
  # control variate step
  if ( (t %% kappa) == 0) {
    cv_w <- w  
    erf_fun <- function(w, z = z_obs, n=n_obs) {
      return( empirical_risk_fun(w, z, n) ) 
    }
    cv_grad_risk <- numDeriv::grad( erf_fun, cv_w ) #controle variate
  }
  # step 2: check for termination 
  if ( t>= Tmax ) {
    Qstop <- 1
  }
}
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
```

