---
title: "Stochastic learning methods"
subtitle: "...on a binary classification problem"
author: "Georgios P. Karagiannis @ MATH3431 Machine Learning and Neural Networks III"
output:
  html_notebook: 
    number_sections: true
  word_document: default
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: default
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{amsmath}
---

<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2023 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Associate Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Machine Learning and Neural Networks III (MATH3431) -->
<!-- which is the material of the course (MATH3431 Machine Learning and Neural Networks III) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Machine_Learning_and_Neural_Networks_III_Epiphany_2023  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->



[Back to README](https://github.com/georgios-stats/Machine_Learning_and_Neural_Networks_III_Epiphany_2023/tree/main/Computer_practical#aim)

```{r}
rm(list=ls())
```


---

***Aim***

+ practice in R,  

+ implement GD, batch/online SGD, AdaGrad, SGD with projection, SVRG algorithms in R

+ refresh logistic regression, with Ridge penalty from term 1  

---

***Reading material***


+ Lecture notes:  
    + Handouts 0, 1, 2, and 3 

+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   

+ Reference of *rmarkdown* (optional, supplementary material):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)

+ Reference for *Latex* (optional, supplementary material):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)  

---

***New software***   

+ R package `base` functions:    
    + `set.seed{base}` 

+ R package `nloptr` functions:    
    + `nloptr{nloptr}` 
    
+ R package `numDeriv` functions:    
    + `grad{numDeriv}` 

```{r}
# call libraries
#install.packages("numDeriv")
library(numDeriv)
#install.packages("nloptr")
library(nloptr)
```

---  

***Initialize R***  


```{r, results="hide"}
# Load R package for printing
library(knitr)
```

```{r}
# Set a seed of the randon number generator
set.seed(2023)
```


# Application: Binary classification problem {-}

Consider the binary classification problem with input $x\in\mathbb{R}$ and output/labels $y\in\{0,1\}$.  

The prediction rule is 
\[
h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}
\] 
where $w\in\mathbb{R}^{2}$ is the unknown parameter we wish to learn, and hence we can consider that the hypothesis class is 

\[
\mathcal{H}=\{w\in\mathbb{R}^{2}\}
\]  

Consider there is available a dataset  
\[
\mathcal{S}_{n}=\left\{ z_{i}=\left(y_{i},x_{i}\right)\right\} _{i=1}^{n}
\]  

with $y_{i}\in\{0,1\}$ and $x_{i}\in\mathbb{R}$.  

Recall that the sampling distribution is  
\[
y|w \sim \text{Bernulli}(h_{w}(x)) \\
h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}
\]

The dataset $\mathcal{S}_{n}=\{z_i=(x_i,y_i)\}$ is  generated from the data generation probability $g(\cdot)$ provided below as a routine. We pretend that we do not know $g(\cdot)$. 

```{r}
data_generating_model <- function(n,w) {
  z <- rep( NaN, times=n*2 )
  z <- matrix(z, nrow = n, ncol = 2)
  z[,1] <- runif(n, min = -10, max = 10)
  p <- w[1] + w[2]*z[,1] 
  p <- exp(p) / (1+exp(p))
  z[,2] <- rbinom(n, size = 1, prob = p)
  return(z)
}
```

Let the dataset $\mathcal{S}_{n}$ has size $n=500$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 500
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true) 
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

The prediction rule is 
\[
h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}
\]  

where $w\in\mathbb{R}^{2}$.

The function **prediction_rule(x,w)** that returns the rule $h$ where $x$ is the input argument and $w$ is the unknown parameter is given below.  

```{r}
prediction_rule <- function(x,w) {
  h <- w[1]+w[2]*x
  h <- exp(h) / (1.0 + exp(h) )
  return (h)
}
```

The likelihood function is 
$$
f\left(y|w\right)=\prod_{i=1}^{n}\left(h_{i}\right)^{y_{i}}\left(1-h_{i}\right)^{1-y_{i}}
$$

We consider a loss function as   

\[
\ell\left(w,z=\left(x,y\right)\right)=-y\log\left(h(x)\right)-\left(1-y\right)\log\left(1-h(x)\right)
\]

The code for the loss function is provided below as **loss_fun(w,z)** that computes the loss function, where $z=(x,y)$ is one example (observation) and $w$ is the unknown parameter. 

```{r}
loss_fun <- function(w,z) {
  x = z[1]
  y = z[2]
  h <- prediction_rule(x,w)
  ell <- -y*log(h) - (1-y)*log(1-h)
  return (ell)
}
```

The Risk function under the data generation model $g$ is 

\[
\begin{align*}
R_{g}\left(w\right)= & \text{E}\left(\ell\left(w,z=\left(x,y\right)\right)\right)\\
= & \text{E}\left(-y\log\left(h\left(w;x\right)\right)-\left(1-y\right)\log\left(1-h\left(w;x\right)\right)\right)
\end{align*}
\]

The Empirical risk function is
\[
\begin{align*}
\hat{R}_{S}\left(w\right) & \frac{1}{n}\sum_{i=1}^{n}\ell\left(w,z_{i}=\left(x_{i},y_{i}\right)\right)\\
= & -\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}\log\left(h(w;x_{i})\right)+\left(1-y\right)\log\left(1-h(w;x_{i})\right)\right)
\end{align*}
\]

The function **empirical_risk_fun(w,z,n)** computes the empirical risk, where $z=(x,y)$ is an example, $w$ is the unknown parameter, and $n$ is the data size is given below. 

```{r}
empirical_risk_fun <- function(w,z,n) {
  x = z[,1]
  y = z[,2]
  R <- 0.0
  for (i in 1:n) {
    R <- R + loss_fun(w,z[i,])
  }
  R <- R / n
  return (R)
}
```

# Stochastic gradient descent preparation {-}  

## Task (given)  

Code a function **learning_rate(t,t0)** that computes the learning rate sequence 
\[
\eta_{t}=\frac{t_0}{t}
\]  
where $t$ is the iteration stage and $t_0$ is a constant. 

Use $t_0=3$ as default value.  

```{r}
learning_rate <-function(t,t0=3) {
  eta <- t0 / t
  return( eta )
}
```


## Task (given) 

Code the function **grad_loss_fun(w,z)** that returns the gradient of the loss function at parameter value $w$, and at example value $z=(x,y)$.   

```{r}
grad_loss_fun <- function(w,z) {
  x = z[1]
  y = z[2]
  h <- prediction_rule(x,w)
  grd <- c(h-y, (h-y)*x)
  return (grd)
}
```

## Task (given) 

Code the function **grad_risk_fun <- function(w,z,n)** that returns the gradient of the risk function at parameter value $w$, and using the data set $z$ of size $n\times 2$.    

```{r}
grad_risk_fun <- function(w,z,n) {
  grd <- 0.0
  for (i in 1:n) {
    grd <- grd + grad_loss_fun(w,z[i,])
  }
  grd <- grd / n
  return (grd)
}

```


## Task (for the computer practical)   

Compute the gradient of the empirical risk function at point $w=(-0.1,1.5)^\top$.  

Use the whole dataset $\{z_{i};i=1,...,n\}$ (set of examples). 

Do this by using the command 'grad_risk_fun' provided above.

```{r}
#
#
#
```

## Task (for the computer practical)  

Compute the gradient of the empirical risk function at point $w=(-0.3,3)^\top$. Use the whole dataset $\{z_{i};i=1,...,n\}$ (set of examples).   Do this by using the function 'grad{numDeriv}' from the R package numDeriv. 

E.g., you can use it as numDeriv::grad( fun, w ). You can try ?grad for more info.

```{r}
#
#
#
```

# Gradient descent  

## Task (for the computer practical)  

Code a Gradient Descent (GD) algorithm with constant learning rate $\eta_{t}=0.1$ that returns the chain of all the  $\{w^{(t)}\}$ produced. 

The termination criterion is such that the iterations stop when the the total number of iterations excesses $T=1000$. 

Use seed $w^{(0)}=(-0.3,3)^\top$.   

You may use the R function **grad{numDeriv}** to numerically compute the gradient;  

+ e.g. numDeriv::grad( erf_fun, w ) .  

+ Try ?grad for more info. 

```{r}
#
#
#
```




##  Task (for the computer practical)  

Plot the chain $\{w_1^{(t)}\}$ against the iteration $t$.   

```{r}
#
#
#
```

Plot the chain $\{w_2^{(t)}\}$ against the iteration $t$.  

```{r}
#
#
#
```


## Task (for the computer practical)  

Re-run the previous GD by changing the algorithminc parameter values for $\eta$ for some in the range $(0.001,1.0)$.  

Check how the algorithm behaves by ploting the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$ against the iteration $t$.    

If necessary change the termination criterion to consider more or less iterations.  


```{r}
#
#
#
```

## Task (for homework practice)   

Re run GD by using a learning rate sequence of the form $\eta_t = t0/t$ for different values of $t_0>0$ that you will choose.  

Check how the algorithm behaves by ploting the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$ against the iteration $t$.  

If necessary change the termination criterion to consider more or less iterations.


```{r}
#
#
#
```


# Batch Stochastic Gradient Descent 

Let the data set $\mathcal{S}_{n}$ has size $n=1000000$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stored in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task (for the computer practical)  

Code a batch Stochastic Gradient Descent (GD) algorithm with learning rate $\eta_{t}=0.1$ and batch size $m=10$ that returns the chain of $\{w^{(t)}\}$.  

The batch sampling may be performed as a sampling with replacement (see ?sample.int).  

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
#
#
#
```


## Task (for homework practice)   

Re run the batch SGD by experimenting and changing the values of the learning rate $\eta$ and that of the batch size $m$.  

Plot the produced chains of $\{w^{(t)}\}$. 

What is the impact of the the learning rate $eta$ and that of the batch size $m$ to the noise and the speed of the convergence ?


```{r}
#
#
#
```



# AdaGrad  (for homework practice)     

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task (for homework practice)   

Code an online AdaGrad algorithm with constant$\eta_{t}=1.0$ and batch size $m=1$ that returns the chain of $\{w^{(t)}\}$. 

Use $\epsilon=10^{-6}$

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
#
#
#
```


## Task (for homework practice)   

Code an batch AdaGrad algorithm with $\eta_{t}$ and batch size $m$ that returns the chain of $\{w^{(t)}\}$. 

Experiment by changing the values of $\eta_{t}$  and $m$. See how the oscillations and the convergence of the chain$\{w^{(t)}\}$ behaves.

Use $\epsilon=10^{-6}$

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
#
#
#
```



# Batch Stochastic Gradient Descent with projection  (for homework practice)   

Let the data set $\mathcal{S}_{n}$ has size $n=1000000$.  

The data set containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

Assume that the hypothesis class is restricted such as $\mathcal{H}=\{w\in\mathbb{R}^{2}:|w|_2 \le 2.0\} $

You may use the function 

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```


## Task (Exercise for homework practice)   


Compute $w^{*}$ from the minimization problem 

$$
w^{*}=\arg\min_{w\in\mathcal{H}}\left( F(w) \right)
$$
where $F(w)=\left\Vert w-w' \right\Vert$, $w'=(-0.1,0.3)^\top$ and  $\mathcal{H}=\{w\in\mathbb{R}^{2}:|w|_2 \le 2.0\} $. 

Use the function 'nloptr{nloptr}' from the R package 'nloptr'. Try ?nloptr for more information.  

A set of sufficient arguments for the function to run is given below.

```{r}
# out <- nloptr(x0=..,   
#               eval_f=..., #   
#               eval_grad_f=...,  
#               eval_g_ineq = ...,  
#               eval_jac_g_ineq = ...,   
#               w_now=...,  
#               opts = list("algorithm" = "NLOPT_LD_MMA",  
#                           "xtol_rel"=1.0e-8)  
# out$solution
```


The following functions are provided.

```{r}
#
boundary <- 2.0 # this is the value |w|_{2}^{2} <= boundary
# auxiliary functions to compute the projection
eval_f0 <- function( w_proj, w_now ){ 
  return( sqrt(sum((w_proj-w_now)^2)) )
}
eval_grad_f0 <- function( w, w_now ){ 
  return( c( 2*(w[1]-w_now[1]), 2*(w[2]-w_now[2]) ) )
}
eval_g0 <- function( w_proj, w_now) {
  return( sum(w_proj^2) -(boundary)^2 )
}
eval_jac_g0 <- function( x, w_now ) {
  return(   c(2*w[1],2*w[2] )  )
}
```

Write your code below

```{r}
#
#
#
```


## Task  (for homework practice)     

Code a batch Stochastic Gradient Descent (SGD) algorithm with learning rate $\eta_{t}=0.01$, batch size $m=1$ and projection to $\mathcal{H}=\{w:|w|_2\le 2.0\}$, that returns the chain of $\{w^{(t)}\}$. 

The batch sampling may be performed as a sampling with replacement (see ?sample.int). 

The termination criterion is when the total number of iterations excesses $T=5000$. Seed with $w^{(0)}=(-0.1,0.3)^\top$.   

You may use the function 'nloptr{nloptr}' from the R package 'nloptr'. Try ?nloptr for more information.


```{r}
#
boundary <- 2.0 # this is the value |w|_{2}^{2} <= boundary
# auxiliary functions to compute the projection
eval_f0 <- function( w_proj, w_now ){ 
  return( sqrt(sum((w_proj-w_now)^2)) )
}
eval_grad_f0 <- function( w, w_now ){ 
  return( c( 2*(w[1]-w_now[1]), 2*(w[2]-w_now[2]) ) )
}
eval_g0 <- function( w_proj, w_now) {
  return( sum(w_proj^2) -(boundary)^2 )
}
eval_jac_g0 <- function( x, w_now ) {
  return(   c(2*w[1],2*w[2] )  )
}
```


```{r}
#
#
#
```

---

# Stochastic Variance Reduced Gradient Descent (SVRG)  (for homework practice)   

## Task  (for homework practice)    

Code a Stochastic Variance Reduced Gradient (SVRG) Descent algorithm with learning rate $\eta_{t}=0.1$ and batch size $m=1$ (namely the online SGD version of it) that returns the chain of $\{w^{(t)}\}$.  

Consider $\kappa=100$ as the SVRG parameter controlling the number of snapshots.   

The sampling may be performed as a sampling with replacement (see ?sample.int).  

The termination criterion is when the total number of iterations excesses $T=1000$. Seed with $w^{(0)}=(-0.3,3)^\top$.  

Produce the trace plots of the produced chains $\{w^(t)\}$.  


```{r}
#
#
#
```

## Task (for homework practice)    

Repeat the above task by changing the parameters $\kappa$ in order to see how changing $\kappa$ affects the convergence and the noise of the chain. 



```{r}
#
#
#
```




