---
title: "Stochastic learning methods"
subtitle: "Case study: enzyme data-set"
author: "Georgios P. Karagiannis @ MATH3431 Machine Learning and Neural Networks III"
output:
  html_document:
    df_print: paged
    number_sections: true
  word_document: default
  pdf_document: default
  html_notebook: 
    number_sections: true
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{amsmath}
---

<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2023 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Associate Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Machine Learning and Neural Networks III (MATH3431) -->
<!-- which is the material of the course (MATH3431 Machine Learning and Neural Networks III) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Machine_Learning_and_Neural_Networks_III_Epiphany_2023  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->



[Back to README](https://github.com/georgios-stats/Machine_Learning_and_Neural_Networks_III_Epiphany_2023/tree/main/Computer_practical#aim)

```{r}
rm(list=ls())
```



---

***Aim***

Students will become able to:  

+ practice in R,  

+ implement GD, batch/online SGD, AdaGrad, SGD with projection, SVRG algorithms in R with possibility to extend to other programming languadges  

+ refresh logistic regression, with Ridge penalty  

---

***Reading material***


+ Lecture notes:  
    + Handouts 0, 1, 2, and 3 

+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   

+ Reference of *rmarkdown* (optional):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)

+ Reference for *Latex* (optional):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)  

---

***New software***   

+ R package `base` functions:    
    + `set.seed{base}` 

+ R package `nloptr` functions:    
    + `nloptr{nloptr}` 
    
+ R package `numDeriv` functions:    
    + `grad{numDeriv}` 

```{r}
# call libraries
library(numDeriv)
library(nloptr)
```

---


```{r, results="hide"}
# Load R package for printing
library(knitr)
```

```{r, results="hide"}
# Set a seed of the randon number generator
set.seed(2023)
```


# Application: Logistic regression    {-}

Consider the Logistic regression problem with input $x\in\mathbb{R}$ and output/labels $y\in\{0,1\}$. The prediction rule is $$h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}$$, where $w\in\mathbb{R}^{2}$ is the unknown parameter we wish to learn, and hence we can consider that the hypothesis class is $$\mathcal{H}=\{w\in\mathbb{R}^{2}\}$$.  

Consider there is available a dataset  $$\mathcal{S}_{n}=\left\{ z_{i}=\left(y_{i},x_{i}\right)\right\} _{i=1}^{n}$$ , with $y_{i}\in\{0,1\}$ and $x_{i}\in\mathbb{R}$.  Recall that the sampling distribution is  
$$
y|w \sim \text{Bernulli}(h_{w}(x)) \\
h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}
$$

The dataset $\mathcal{S}_{n}$ is  generated from the data generation probability $g(\cdot)$ provided below as a routine. We pretend that we do not know $g(\cdot)$. 

```{r}
data_generating_model <- function(n,w) {
  z <- rep( NaN, times=n*2 )
  z <- matrix(z, nrow = n, ncol = 2)
  z[,1] <- runif(n, min = -10, max = 10)
  p <- w[1] + w[2]*z[,1] 
  p <- exp(p) / (1+exp(p))
  z[,2] <- rbinom(n, size = 1, prob = p)
  return(z)
}
```


Let the dataset $\mathcal{S}_{n}$ has size $n=500$.  Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  
```{r}
set.seed(2023)
n_obs <- 500
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true) 
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```



The likelihood function is 
$$
f\left(y|w\right)=\prod_{i=1}^{n}\left(h_{i}\right)^{y_{i}}\left(1-h_{i}\right)^{1-y_{i}}
$$

We consider a loss function as 
$$
\ell\left(w,z=\left(x,y\right)\right)=-y\log\left(h(x)\right)-\left(1-y\right)\log\left(1-h(x)\right)
$$

The Risk function under the data generation model $g$ is 
$$
\begin{align*}
R_{g}\left(w\right)= & \text{E}\left(\ell\left(w,z=\left(x,y\right)\right)\right)\\
= & \text{E}\left(-y\log\left(h\left(w;x\right)\right)-\left(1-y\right)\log\left(1-h\left(w;x\right)\right)\right)
\end{align*}
$$


## Task 1 {-}

The prediction rule is $$h_{w}(x) = \frac {\exp(w^\top x)}{1+\exp(w^\top x)}$$, where $w\in\mathbb{R}^{2}$

Write a function `prediction_rule(x,w)' that returns the rule $h$ where $x$ is the input argument and $w$ is the unknown parameter.

```{r}
prediction_rule <- function(x,w) {
  h <- w[1]+w[2]*x
  h <- exp(h) / (1.0 + exp(h) )
  return (h)
}
```



## Task 2 {-} 


The loss function as 
$$
\ell\left(w,z=\left(x,y\right)\right)=-y\log\left(h(x)\right)-\left(1-y\right)\log\left(1-h(x)\right)
$$
Write a function `loss_fun(w,z)' that computes the loss function, where $z=(x,y)$ is one example (observation) and $w$ is the unknown parameter. 

```{r}
loss_fun <- function(w,z) {
  x = z[1]
  y = z[2]
  h <- prediction_rule(x,w)
  ell <- -y*log(h) - (1-y)*log(1-h)
  return (ell)
}
```

## Task 3 {-} 

The Empirical risk function is
$$
\begin{align*}
\hat{R}_{S}\left(w\right) & \frac{1}{n}\sum_{i=1}^{n}\ell\left(w,z_{i}=\left(x_{i},y_{i}\right)\right)\\
= & -\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}\log\left(h(w;x_{i})\right)+\left(1-y\right)\log\left(1-h(w;x_{i})\right)\right)
\end{align*}
$$

Write a function `empirical_risk_fun(w,z,n)' that computes the empirical risk, where $z=(x,y)$ is an example, $w$ is the unknown parameter, and $n$ is the data size. 

```{r}
empirical_risk_fun <- function(w,z,n) {
  x = z[,1]
  y = z[,2]
  R <- 0.0
  for (i in 1:n) {
    R <- R + loss_fun(w,z[i,])
  }
  R <- R / n
  return (R)
}
```

## Task 4 {-} 

Write a function `learning_rate(t,t0)' that computes the learning rate sequence $$\eta_{t}=\frac{t_0}{t}$$ where $t$ is the iteration stage and $t_0$ is a constant. Use $t_0=3$ as default value.  

```{r}
learning_rate <-function(t,t0=3) {
  eta <- t0 / t
  return( eta )
}
```

## Task 5 {-} 

Code the function 'grad_loss_fun(w,z)' that returns the gradient of the loss function at parameter value w, and at example value z. 

```{r}
grad_loss_fun <- function(w,z) {
  x = z[1]
  y = z[2]
  h <- prediction_rule(x,w)
  grd <- c(h-y, (h-y)*x)
  return (grd)
}
```

## Task 6 {-} 

Code the function 'grad_risk_fun <- function(w,z,n)' that returns the gradient of the risk function at parameter value $w$, and using the dataset $z$ of size $n\times 2$ .  

```{r}
grad_risk_fun <- function(w,z,n) {
  grd <- 0.0
  for (i in 1:n) {
    grd <- grd + grad_loss_fun(w,z[i,])
  }
  grd <- grd / n
  return (grd)
}

```

# Task: Gradient descent

## Task 

Code a Gradient Descent (GD) algorithm with learning rate $\eta_{t}=0.1$ that returns the chain of $\{w^{(t)}\}$. 

The termination criterion is when the total number of iterations excesses $T=1000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

You may use the R function 'grad{numDeriv}' to numerically compute the gradient. Try ?grad .


```{r}
#
#
#
```

## Task 

Plot the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$


```{r}
#
#
#
```

## Task 

Repeat previous couple of tasks to run GD by changing the values of $\eta$ in the range $(0.001,1.0)$. Check how the algorithm behaves by ploting the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$. If necessary change the termination criterio to consider more or less iterations.

```{r}
#
#
#
```

## Task 

Repeat previous couple of tasks to run GD by using a learning rate sequence of the form $\eta_t = t0/t$ for different values of $t_0>0$ that you will choose. Check how the algorithm behaves by ploting the chains $\{w_1^{(t)}\}$ and $\{w_2^{(t)}\}$. If necessary change the termination criterion to consider more or less iterations.


```{r}
#
#
#
```

# Batch Stochastic Gradient Descent 

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task 

Code a batch Stochastic Gradient Descent (GD) algorithm with learning rate $\eta_{t}=0.01$ and batch size $m=10$ that returns the chain of $\{w^{(t)}\}$. The batch sampling may be performed as a sampling with replacement (see ?sample.int). The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
#
#
#
```


## Task 

Repeat the task above by experimenting and changing the values of the learning rate $eta$ and that of the batch size $m$. Plot the produced chains. What is the impact of the the learning rate $eta$ and that of the batch size $m$ to the noise and the speed of the convergence ?


```{r}
#
#
#
```



# AdaGrad  

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task 

Code an online AdaGrad algorithm with $\eta_{t}=1.0$ and batch size $m=1$ that returns the chain of $\{w^{(t)}\}$. 

Use $\epsilon=10^{-6}$

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
#
#
#
```


## Task 

Code an batch AdaGrad algorithm with $\eta_{t}$ and batch size $m$ that returns the chain of $\{w^{(t)}\}$. 

Experiment by changing the values of $\eta_{t}$  and $m$. See how the oscillations and the convergence of the chain$\{w^{(t)}\}$ behaves.

Use $\epsilon=10^{-6}$

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.3,3)^\top$.   

```{r}
#
#
#
```



# Batch Stochastic Gradient Descent with projection

Let the dataset $\mathcal{S}_{n}$ has size $n=1000000$.  

The dataset containing the examples to train the model are generated below, and stores in the array $z_{\text{obs}}$.  

Assume that the real values for the unknown parameters $w$ is $w_{\text{true}}=(0.0,1.0)^\top$.  

Assume that the hypothesis class is restricted such as $\mathcal{H}=\{w\in\mathbb{R}^{2}:|w|_2 \le 2.0\} $

You may use the function 

```{r}
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)  
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
```

## Task 

Code a batch Stochastic Gradient Descent (GD) algorithm with learning rate $\eta_{t}=0.01$, batch size $m=1$ and projection to $\mathcal{H}$, that returns the chain of $\{w^{(t)}\}$. 

The batch sampling may be performed as a sampling with replacement (see ?sample.int). 

The termination criterion is when the total number of iterations excesses $T=10000$. Seed with $w^{(0)}=(-0.1,0.3)^\top$.   

You may use the function 'nloptr{nloptr}' from the R package 'nloptr'. Try ?nloptr .


```{r}
#
boundary <- 2.0 # this is the value |w|_{2}^{2} <= boundary
# auxiliary functions to compute the projection
eval_f0 <- function( w_proj, w_now ){ 
  return( sqrt(sum((w_proj-w_now)^2)) )
}
eval_grad_f0 <- function( w, w_now ){ 
  return( c( 2*(w[1]-w_now[1]), 2*(w[2]-w_now[2]) ) )
}
eval_g0 <- function( w_proj, w_now) {
  return( sum(w_proj^2) -(boundary)^2 )
}
eval_jac_g0 <- function( x, w_now ) {
  return(   c(2*w[1],2*w[2] )  )
}
```


```{r}
#
#
#
```

